{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize folders and files\n",
    "folders = ['data', 'geo']\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# For writing rows line by line\n",
    "def write_csv_row(filename, columns, row):\n",
    "    '''\n",
    "    Helper function to avoid redundant code. Basically checks if there is an existing csv file, then writes the data in real time.\n",
    "\n",
    "    Args:\n",
    "        filename (str): path of the csv file.\n",
    "        columns (iterable): list of names to be used as column headers.\n",
    "        row (iterable): actual row data.\n",
    "\n",
    "    Returns: none\n",
    "    '''\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=columns)\n",
    "        if not file_exists or os.stat(filename).st_size == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct API Requests - Extraction of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_locals_from_comelec(code, name):\n",
    "    ''' Fetches the JSON data for the Regional down to baranggay level data. '''\n",
    "    \n",
    "    url = f\"https://2025electionresults.comelec.gov.ph/data/regions/local/{code}.json\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "        'Cookie': '__cf_bm=dodswbwnKOQkXT6kGyV5hLFBczyHlWBFAg7P.Mq6dR8-1747135983-1.0.1.1-5gWqGu78cTooG.DZ91TAvT5YOQHCc6n1lhirFiq064M80KYfYva.HyXpMY9ebkMcxC4wKn7h4dTgYJZBmvl1G9wGa599RmG4nXo.VWKGHCQ',\n",
    "        'sec-ch-ua-platform': 'Windows',\n",
    "        'Referer': 'https://2025electionresults.comelec.gov.ph/er-result',\n",
    "        'sec-ch-ua': 'Chromium;v=136, Microsoft',\n",
    "        'sec-ch-ua-mobile': '?0'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        data = {}\n",
    "    else:\n",
    "        data = response.json()['regions']\n",
    "        \n",
    "    print(f\"[{response.status_code}] {code} - {name}: Fetched {len(data)} records\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def fetch_precincts_from_comelec(code, name):\n",
    "    ''' \n",
    "    Fetches the JSON data for the precinct-level data. \n",
    "    The request URL structure is quite different from the regional to baranggay data. \n",
    "    '''\n",
    "\n",
    "    url = f\"https://2025electionresults.comelec.gov.ph/data/regions/precinct/{code[:2]}/{code}.json\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'if-modified-since': 'Tue, 13 May 2025 12:50:19 GMT',\n",
    "    'if-none-match': 'W/38c86a4b725b8f16a24b7d650f0d1a21',\n",
    "    'priority': 'u=1, i',\n",
    "    'referer': 'https://2025electionresults.comelec.gov.ph/er-result',\n",
    "    'sec-ch-ua': 'Chromium;v=136, Microsoft',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': 'Windows',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-origin'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        data = {}\n",
    "    else:\n",
    "        data = response.json()['regions']\n",
    "        \n",
    "    print(f\"[{response.status_code}] {code} - {name}: Fetched {len(data)} records\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def fetch_er_data(code):\n",
    "    ''' Fetches the JSON data for the precinct vote counts. '''\n",
    "\n",
    "    url = f\"https://2025electionresults.comelec.gov.ph/data/er/{code[:3]}/{code}.json\"\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'priority': 'u=1, i',\n",
    "    'referer': 'https://2025electionresults.comelec.gov.ph/er-result',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"136\", \"Microsoft Edge\";v=\"136\", \"Not.A/Brand\";v=\"99\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-origin'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        data = {}\n",
    "        print(f\"[{response.status_code}] Precinct {code}: no data found\")\n",
    "    else:\n",
    "        data = response.json()\n",
    "        print(f\"[{response.status_code}] Precinct {code} - {data['information']['precinctInCluster']}: OK\")\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the granular data (Geographical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the regions\n",
    "def get_regional_data():\n",
    "    ''' Initialize the top level list, outputs the regional-level data. '''\n",
    "\n",
    "    regions = fetch_locals_from_comelec('0', 'region')\n",
    "    pd.DataFrame(regions).to_csv('./geo/regions.csv')\n",
    "\n",
    "    return regions\n",
    "\n",
    "# Loop through the regions and get the province codes, etc.\n",
    "def fetch_local_data(data, target):\n",
    "    ''' Loop through higher level data and drill down to fetch granular data '''\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    for d in data:\n",
    "        category_code = d['categoryCode']\n",
    "        parent_code = d['masterCode']\n",
    "        current_code = d['code']\n",
    "        name = d['name']\n",
    "\n",
    "        target_data = fetch_locals_from_comelec(current_code, name)\n",
    "\n",
    "        output.extend(target_data)\n",
    "    \n",
    "    print(f\"{len(output)} total records\")\n",
    "    print(output)\n",
    "    \n",
    "    pd.DataFrame(output).to_csv(f\"./geo/{target}.csv\")\n",
    "\n",
    "    return output\n",
    "\n",
    "# Loop through the baranggays and get the province codes, etc.\n",
    "def fetch_precinct_data():\n",
    "    ''' Loop through baranggay data and drill down to fetch precint-level data. Used the csv file to avoid out of memory errors. '''\n",
    "\n",
    "    baranggays = []\n",
    "    with open('./geo/baranggays.csv', mode='r', newline='') as f:\n",
    "        reader = csv.reader(f) #returns a generator\n",
    "        next(reader) #skip the headers\n",
    "\n",
    "        for row in reader:\n",
    "            baranggays.append(row[3])\n",
    "\n",
    "    with open('./geo/recincts.csv', mode='w', newline='') as f:\n",
    "        columns = ['categoryCode', 'masterCode', 'code', 'name']\n",
    "        writer = csv.DictWriter(f, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Loop through the baranggays\n",
    "        for baranggay in baranggays:\n",
    "            current_code = baranggay\n",
    "            precincts = fetch_precincts_from_comelec(current_code, 'precincts')\n",
    "\n",
    "            for precinct in precincts:\n",
    "                writer.writerow(\n",
    "                    {\n",
    "                        'categoryCode': None, \n",
    "                        'masterCode': current_code,\n",
    "                        'code': precinct['code'],\n",
    "                        'name': precinct['name']\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# Master function\n",
    "def main():\n",
    "    regions = get_regional_data()\n",
    "    provinces = fetch_local_data(regions, 'provinces')\n",
    "    cities = fetch_local_data(provinces, 'cities')\n",
    "    baranggays = fetch_local_data(cities, 'baranggays')\n",
    "    precincts = fetch_precinct_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Functions (Actual vote counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL functions\n",
    "def extract_precinct_data(data):\n",
    "    precinct_info = data['information']\n",
    "    senator_info = data['national'][0]['statistic']\n",
    "    party_info = data['national'][1]['statistic']\n",
    "\n",
    "    row = {\n",
    "        \"precinct_code\" : precinct_info['precinctId'],\n",
    "        \"precinct_cluster\" : precinct_info['precinctInCluster'],\n",
    "        \"location\" : precinct_info['location'],\n",
    "        \"abstentions\" : precinct_info['abstentions'],\n",
    "        \"registered_voters\" : precinct_info['numberOfRegisteredVoters'],\n",
    "        \"actual_voters\" : precinct_info['numberOfActuallyVoters'],\n",
    "        \"valid_ballots\" : precinct_info['numberOfValidBallot'],\n",
    "\n",
    "        \"senator_over\" : senator_info['overVotes'],\n",
    "        \"senator_under\" : senator_info['underVotes'],\n",
    "        \"senator_valid\" : senator_info['validVotes'],\n",
    "        \"senator_obtained\" : senator_info['obtainedVotes'],\n",
    "\n",
    "        \"party_over\" : party_info['overVotes'],\n",
    "        \"party_under\" : party_info['underVotes'],\n",
    "        \"party_valid\" : party_info['validVotes'],\n",
    "        \"party_obtained\" : party_info['obtainedVotes']    \n",
    "    }\n",
    "\n",
    "    columns = list(row.keys())\n",
    "    write_csv_row('./data/precinct_info.csv', columns, row)        \n",
    "\n",
    "def extract_senator_data(data):\n",
    "    precinct_code = data['information']['precinctId']\n",
    "    senators = data['national'][0]['candidates']['candidates']\n",
    "\n",
    "    columns = ['precinct_code', 'name', 'vote']\n",
    "\n",
    "    for senator in senators:\n",
    "        row = {\n",
    "            'precinct_code' : precinct_code,\n",
    "            'name' : senator['name'],\n",
    "            'vote' : senator['votes']\n",
    "        }\n",
    "\n",
    "        write_csv_row('./data/precinct_senators_temp.csv', columns, row)\n",
    "\n",
    "\n",
    "def extract_partylist_data(data):\n",
    "    precinct_code = data['information']['precinctId']\n",
    "    partylists = data['national'][1]['candidates']['candidates']\n",
    "\n",
    "    columns = ['precinct_code', 'name', 'vote']\n",
    "\n",
    "    for partylist in partylists:\n",
    "        row = {\n",
    "            'precinct_code' : precinct_code,\n",
    "            'name' : partylist['name'],\n",
    "            'vote' : partylist['votes']\n",
    "        }\n",
    "\n",
    "        write_csv_row('./data/precinct_partylist_temp.csv', columns, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data():\n",
    "    # Read the csv files in their current state\n",
    "    df_party = pd.read_csv('./data/precinct_partylist.csv', encoding_errors=\"ignore\").drop_duplicates()\n",
    "    df_senator = pd.read_csv('./data/precinct_senators.csv', encoding_errors=\"ignore\").drop_duplicates()\n",
    "    pendings = pd.read_csv('PENDING.csv')\n",
    "\n",
    "    # Precinct data\n",
    "    all_precincts = pd.read_csv('./geo/precincts.csv', encoding_errors=\"ignore\", low_memory=False)['code']\n",
    "    curr_precincts = pd.read_csv('./data/precinct_info.csv', encoding_errors=\"ignore\", low_memory=False)['precinct_code'].drop_duplicates()\n",
    "    empty_precincts = pd.read_csv('empty_precincts.csv', encoding_errors=\"ignore\", low_memory=False)['code']\n",
    "\n",
    "    # check which precinct data is not yet downloaded, overwrite the file\n",
    "    pending_info = all_precincts[(~all_precincts.isin(curr_precincts)) & (~all_precincts.isin(empty_precincts))]\n",
    "    pending_senator = all_precincts[(~all_precincts.isin(df_senator['precinct_code'].unique()))].drop_duplicates()\n",
    "    pending_senator = pending_senator[(~pending_senator.isin(empty_precincts))]\n",
    "    pending_party = all_precincts[(~all_precincts.isin(df_party['precinct_code'].unique()))].drop_duplicates()\n",
    "    pending_party = pending_party[(~pending_party.isin(empty_precincts))]\n",
    "\n",
    "    # Get all the missing data for info + senators + partylist, and load to the PENDINGS.csv\n",
    "    # Delete all the duplicate data\n",
    "    pendings = pd.concat([pending_info, pending_senator, pending_party], ignore_index=True).drop_duplicates()\n",
    "    pendings.to_csv('PENDING.csv', index=False)\n",
    "\n",
    "    return pendings, df_party, df_senator\n",
    "\n",
    "def collate(party, senator):\n",
    "    # Collate \n",
    "    party.to_csv('./data/precinct_partylist.csv', index=False)\n",
    "    senator.to_csv('./data/precinct_senators.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_er_data(check_data=False):\n",
    "    if check_data:\n",
    "        check_data()\n",
    "\n",
    "        with open('PENDING.csv', mode='r', newline='') as f:\n",
    "            precinct_codes = [x.split(',')[0].strip() for x in f][1:]\n",
    "\n",
    "    else:\n",
    "        with open('./output/precincts.csv', mode='r', newline='') as f:\n",
    "            precinct_codes = [x.split(',')[2] for x in f][1:]\n",
    "\n",
    "    for idx, code in enumerate(precinct_codes):\n",
    "        er_data = fetch_er_data(code)\n",
    "\n",
    "        if not er_data:\n",
    "            print(f\"Extracted precinct {code} ({idx}/{len(precinct_codes)}) - No Data found\")\n",
    "            write_csv_row('empty_precincts.csv', ['code'], {'code': code})\n",
    "        else:\n",
    "            extract_precinct_data(er_data)\n",
    "            extract_senator_data(er_data)\n",
    "            extract_partylist_data(er_data)\n",
    "\n",
    "            print(f\"Extracted precinct {code} ({idx}/{len(precinct_codes)}) - Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first and wait until completed\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run until main is completed!\n",
    "transform_er_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to merge csv files as one.\n",
    "pending, parties, senators = check_data()\n",
    "if len(pending) == 0:\n",
    "    collate(parties, senators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
